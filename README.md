Below is a **polished, production-quality `README.md`** generated **specifically from your updated files and their actual behavior**:

* `main.py` â†’ **Master GUI launcher**
* `MouseControl.py` â†’ **Nose + blink mouse control**
* `voice_command.py` â†’ **Voice assistant**

This README accurately reflects what the code does today, not aspirational features, and is suitable for **GitHub, portfolio, or research demos**

---

# ğŸ§  Unified Humanâ€“Computer Interaction System

A **hands-free multimodal humanâ€“computer interaction system** that combines
**computer visionâ€“based mouse control** and **voice commands**, orchestrated by a **single master GUI**.

This project enables users to control a computer using:

* ğŸ–±ï¸ **Nose movement** for cursor control
* ğŸ‘ï¸ **Eye blinks** for mouse clicks
* ğŸ™ï¸ **Voice commands** for system and application control

All subsystems are launched, monitored, and terminated from **one master control panel**.

---

## âœ¨ Key Features

### ğŸ¯ Mouse Control (Computer Vision)

* Nose movement â†’ smooth cursor movement
* Left eye blink â†’ left click
* Right eye blink â†’ right click
* Automatic eye-open calibration
* Adaptive acceleration (fast flicks move faster)
* Muted, low-saturation camera UI for reduced eye strain
* Adjustable sensitivity (`+` / `-` keys)

### ğŸ™ï¸ Voice Command Assistant

* Wake-word based activation
* Intent recognition with confidence scoring
* Application launching (Chrome, VS Code, Explorer, etc.)
* Editing & browser shortcuts (copy, paste, undo, new tabâ€¦)
* Context-aware modes (SYSTEM / EDIT / BROWSER)
* Unknown command logging for future learning

### ğŸªŸ Master Control GUI

* Single launcher (`main.py`)
* Start / stop each subsystem independently
* Clean shutdown of child processes
* Prevents camera & microphone conflicts
* Fault-tolerant (each subsystem runs in its own process)

---

## ğŸ—ï¸ Architecture Overview

```
main.py
â”‚
â”œâ”€â”€ MouseControl.py
â”‚   â””â”€â”€ Nose-based cursor + blink detection (OpenCV + MediaPipe)
â”‚
â”œâ”€â”€ voice_command.py
â”‚   â””â”€â”€ Voice assistant (SpeechRecognition + rule-based NLP)
â”‚
â””â”€â”€ OS-level process isolation (subprocess)
```

Each module runs in a **separate Python process**, ensuring:

* Stability
* No shared event loops
* No device contention
* Easy extensibility

---

## ğŸ“ Project Structure

```
project/
â”‚
â”œâ”€â”€ main.py                 # Master GUI launcher
â”œâ”€â”€ MouseControl.py         # Nose + blink mouse control
â”œâ”€â”€ voice_command.py        # Voice assistant
â”œâ”€â”€ unknown_commands.json   # Auto-generated (voice assistant)
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the Repository

```bash
git clone <your-repo-url>
cd project
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install opencv-python mediapipe numpy pyautogui
pip install SpeechRecognition keyboard
```

#### Windows (Microphone Support)

If `pyaudio` fails:

```bash
pip install pipwin
pipwin install pyaudio
```

---

### 3ï¸âƒ£ Run the System

```bash
python main.py
```

This opens the **Master Human Interface Controller**.

From the GUI:

* â–¶ Start **Nose Cursor + Blink**
* â–¶ Start **Voice Assistant**
* â¹ Stop either independently
* ğŸ›‘ Quit all safely

---

## ğŸ® Controls

### ğŸ–±ï¸ Mouse Control (Camera Window)

| Action               | Control         |
| -------------------- | --------------- |
| Cursor move          | Nose movement   |
| Left click           | Left eye blink  |
| Right click          | Right eye blink |
| Increase sensitivity | `+` or `=`      |
| Decrease sensitivity | `-`             |
| Quit mouse control   | `Q`             |

---

### ğŸ™ï¸ Voice Assistant

**Wake words**

* `computer`
* `assistant`
* `hey system`

**Example commands**

* â€œcomputer open chromeâ€
* â€œassistant new tabâ€
* â€œhey system copyâ€
* â€œcomputer pasteâ€
* â€œassistant take screenshotâ€

---

## ğŸ§  Design Philosophy

* Zero-regression integration
* Hardware isolation (camera & mic)
* Human-centric interaction
* Research-ready structure
* Assistive-technology friendly

This architecture is intentionally designed to support:

* Adaptive learning
* Multimodal intent fusion
* Reinforcement-based personalization
* Accessibility research

---

## ğŸ”® Future Enhancements

* Voice-controlled cursor modes (precision / fast)
* Learning-based sensitivity adaptation
* Gaze-only fallback (no nose movement)
* Inter-process intent bus (IPC)
* Health monitoring & auto-restart
* Single-EXE packaging

---

## âš ï¸ Known Limitations

* Requires stable lighting for face tracking
* Microphone quality affects recognition accuracy
* Single-user calibration per session

---

## ğŸ›¡ï¸ Disclaimer

This software controls system-level input (mouse & keyboard).
Use responsibly.
Do **not** run with elevated/admin privileges.

---

## ğŸ‘¤ Author

**Pratik Chopade**
Computer Vision â€¢ Humanâ€“Computer Interaction â€¢ Generative AI

---

If you want next, I can:

* ğŸ“¦ Package this into a **Windows EXE**
* ğŸ§  Add **learning-based adaptation**
* ğŸ”Š Let **voice commands control mouse sensitivity**
* ğŸ“Š Add **real-time diagnostics in the master GUI**

Just tell me the next step ğŸš€
